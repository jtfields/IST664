{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "fastai-with-transformers-bert-roberta.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9qWBUWhldMm",
        "colab_type": "text"
      },
      "source": [
        "# Fastai with HuggingFace ðŸ¤—Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)\n",
        "\n",
        "![fastai + Transformers](https://i.ibb.co/qspmrcm/fastai-transformers-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHC7je2UldMp",
        "colab_type": "text"
      },
      "source": [
        "N.B. This implementation is a suplement of the Medium article [\"Fastai with ðŸ¤—Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)\"](https://medium.com/p/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376).\n",
        "\n",
        "**Also, remember the upvote button is next to the fork button, and it's free too!** ðŸ˜‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZQzYHwJldMp",
        "colab_type": "text"
      },
      "source": [
        "## Introduction : Story of transfer learning in NLP\n",
        "In early 2018, Jeremy Howard (co-founder of fast.ai) and Sebastian Ruder introduced the  [Universal Language Model Fine-tuning for Text Classification](https://medium.com/r/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1801.06146.pdf) (ULMFiT) method. ULMFiT was the first **Transfer Learning** method applied to NLP. As a result, besides significantly outperforming many state-of-the-art tasks, it allowed, with only 100 labeled examples, to match performances equivalent to models trained on 100Ã—  more data.\n",
        "\n",
        "The first time I heard about ULMFiT was during a [fast.ai course](https://course.fast.ai/videos/?lesson=4) given by Jeremy Howard. He demonstrated how it was easy â€Š-â€Š thanks to the ``fastai`` library â€Š-â€Š to implement the complete ULMFit method with only a few lines of codes. In his demo, he used an AWD-LSTM neural network pre-trained on Wikitext-103 and get rapidly state-of-the-art results. He also explained key techniques - also demonstrated in ULMFiT - to fine-tune the models like **Discriminate Learning Rate**, **Gradual Unfreezing** or **Slanted Triangular Learning Rates**.\n",
        "\n",
        "Since the introduction of ULMFiT, **Transfer Learning** became very popular in NLP and yet Google (BERT, Transformer-XL, XLNet), Facebook (RoBERTa, XLM) or even OpenAI (GPT, GPT-2) begin to pre-train their own model on very large corpora. This time, instead of using the AWD-LSTM neural network, they all used a more powerful architecture based on the Transformer (cf. [Attention is all you need](https://arxiv.org/abs/1706.03762)).\n",
        "\n",
        "Although these models are powerful, ``fastai`` do not integrate all of them. Fortunately, [HuggingFace](https://huggingface.co/) ðŸ¤— created the well know [transformers library](https://github.com/huggingface/transformers). Formerly knew as ``pytorch-transformers`` or ``pytorch-pretrained-bert``, this library brings together over 40 state-of-the-art pre-trained NLP models (BERT, GPT-2, RoBERTa, CTRLâ€¦). The implementation gives interesting additional utilities like tokenizer, optimizer or scheduler.\n",
        "\n",
        "The ``transformers`` library can be self-sufficient but incorporating it within the ``fastai`` library provides simpler implementation compatible with powerful fastai tools like  **Discriminate Learning Rate**, **Gradual Unfreezing** or **Slanted Triangular Learning Rates**. The point here is to allow non-NLP-experts to get easily state-of-the-art results and therefore \"make NLP uncool again\".\n",
        "\n",
        "It worth noting that the integration of the HuggingFace ``transformers`` library in ``fastai`` has already been demonstrated in:\n",
        "* Keita Kurita's article [A Tutorial to Fine-Tuning BERT with Fast AI](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/) which makes ``pytorch_pretrained_bert`` library compatible with ``fastai``.\n",
        "* Dev Sharma's article [Using RoBERTa with Fastai for NLP](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) which makes ``pytorch_transformers`` library compatible with ``fastai``.\n",
        "\n",
        "Although these articles are of high quality, note that their demonstration is not compatible anymore with the last version of ``transformers``.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzMlNsW2ldMq",
        "colab_type": "text"
      },
      "source": [
        "## ðŸ›  Integrating transformers with fastai for multiclass classification\n",
        "Before beginning the implementation, note that integrating ``transformers`` within ``fastai`` can be done in multiple different ways. For that reason, I decided to bring simple solutions, that are the most generic and flexible. More precisely, I try to make the minimum of modification in both libraries while making them compatible with the maximum amount of transformer architectures.\n",
        "\n",
        "Note that in addition to this NoteBook and the [Medium article](https://medium.com/p/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376), I made another version available on my GitHub(TODO add link)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQtpOG_ldMr",
        "colab_type": "text"
      },
      "source": [
        "### Libraries Installation\n",
        "Before starting the implementation, you will need to install the ``fastai`` and ``transformers`` libraries. To do so, just follow the instructions [here](https://github.com/fastai/fastai/blob/master/README.md#installation) and [here](https://github.com/huggingface/transformers#installation).\n",
        "\n",
        "In Kaggle, the ``fastai`` library is already installed. So you just have to instal ``transformers`` with :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf3s4AVlldMs",
        "colab_type": "code",
        "outputId": "02acb33d-0467-4056-920e-e89e4396fb19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "%%bash\n",
        "pip install transformers"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.11.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.83)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.18)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.18)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "t_F_Ca2KldMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from pathlib import Path \n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import random \n",
        "\n",
        "# fastai\n",
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from fastai.callbacks import *\n",
        "\n",
        "# transformers\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
        "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a68EXVO3ldMy",
        "colab_type": "text"
      },
      "source": [
        "The current versions of the fastai and transformers libraries are respectively 1.0.58 and 2.1.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8bFy0dildMy",
        "colab_type": "code",
        "outputId": "c681a356-ca37-4141-b1ec-5e342061d8a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import fastai\n",
        "import transformers\n",
        "print('fastai version :', fastai.__version__)\n",
        "print('transformers version :', transformers.__version__)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fastai version : 1.0.59\n",
            "transformers version : 2.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziSE2-OyldM0",
        "colab_type": "text"
      },
      "source": [
        "### ðŸŽ¬ The exampleÂ task\n",
        "The chosen task is a multi-class text classification on [Movie Reviews](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/overview).\n",
        "\n",
        "For each text movie review, the model has to predict a label for the sentiment. We evaluate the outputs of the model on classification accuracy. The sentiment labels are:\n",
        "* 0 â†’ Negative\n",
        "* 1 â†’ Somewhat negative\n",
        "* 2 â†’ Neutral\n",
        "* 3 â†’ Somewhat positive\n",
        "* 4 â†’ Positive\n",
        "\n",
        "The data is loaded into a ``DataFrame`` using ``pandas``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "G1rDrWu9ldM1",
        "colab_type": "code",
        "outputId": "d64e5170-0b0d-422f-fbec-45269717addb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "#DATA_ROOT = Path(\"/content/\") / \"input/sentiment-analysis-on-movie-reviews\"\n",
        "train = pd.read_csv(\"/content/train.tsv\", sep=\"\\t\")\n",
        "test = pd.read_csv(\"/content/test.tsv\", sep=\"\\t\")\n",
        "print(train.shape,test.shape)\n",
        "train.head()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 4) (66292, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "creyQVXIldM3",
        "colab_type": "text"
      },
      "source": [
        "It is worth noting that in the dataset there are no individual movie reviews but rather phrases taken out of context and split into smaller parts, each with an assigned sentiment label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptJKuS5HldM4",
        "colab_type": "text"
      },
      "source": [
        "### Main transformers classes\n",
        "In ``transformers``, each model architecture is associated with 3 main types of classes:\n",
        "* A **model class** to load/store a particular pre-train model.\n",
        "* A **tokenizer class** to pre-process the data and make it compatible with a particular model.\n",
        "* A **configuration class** to load/store the configuration of a particular model.\n",
        "\n",
        "For example, if you want to use the Bert architecture for text classification, you would use [``BertForSequenceClassification``](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification) for the **model class**, [``BertTokenizer``](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer) for the **tokenizer class** and [``BertConfig``](https://huggingface.co/transformers/model_doc/bert.html#bertconfig) for the **configuration class**.Â \n",
        "\n",
        "In order to switch easily between classes â€Š-â€Š each related to a specific model type â€Š-â€Š I created a dictionary that allows loading the correct classes by just specifying the correct model type name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuBZckI0ldM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
        "    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
        "    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n",
        "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
        "    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b6Euyx9ldM6",
        "colab_type": "text"
      },
      "source": [
        "You will see later, that those classes share a common class method ``from_pretrained(pretrained_model_name,Â ...)``. In our case, the parameter ``pretrained_model_name`` is a string with the shortcut name of a pre-trained model/tokenizer/configuration to load, e.g ``'bert-base-uncased'``. We can find all the shortcut names in the transformers documentation [here](https://huggingface.co/transformers/pretrained_models.html#pretrained-models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-NVzNwoldM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "seed = 42\n",
        "use_fp16 = False\n",
        "bs = 16\n",
        "\n",
        "model_type = 'roberta'\n",
        "pretrained_model_name = 'roberta-base' # 'roberta-base-openai-detector'\n",
        "\n",
        "# model_type = 'bert'\n",
        "# pretrained_model_name='bert-base-uncased'\n",
        "\n",
        "# model_type = 'distilbert'\n",
        "# pretrained_model_name = 'distilbert-base-uncased-distilled-squad'#'distilbert-base-uncased'#'distilbert-base-uncased'\n",
        "\n",
        "#model_type = 'xlm'\n",
        "#pretrained_model_name = 'xlm-clm-enfr-1024'\n",
        "\n",
        "#model_type = 'xlnet'\n",
        "#pretrained_model_name = 'xlnet-base-cased'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoSCPGRbldM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVuB787JldM-",
        "colab_type": "code",
        "outputId": "b42db868-8f2b-4a83-d4a3-b5086b76d019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "model_class.pretrained_model_archive_map.keys()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['roberta-base', 'roberta-large', 'roberta-large-mnli', 'distilroberta-base', 'roberta-base-openai-detector', 'roberta-large-openai-detector'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaxGyIP7ldNB",
        "colab_type": "text"
      },
      "source": [
        "It is worth noting that in this case, we use the ``transformers`` library only for a multi-class text classification task. For that reason, this tutorial integrates only the transformer architectures that have a model for sequence classification implemented. These model types areÂ :\n",
        "* BERT (from Google)\n",
        "* XLNet (from Google/CMU)\n",
        "* XLM (from Facebook)\n",
        "* RoBERTa (from Facebook)\n",
        "* DistilBERT (from HuggingFace)\n",
        "\n",
        "However, if you want to go furtherâ€Š-â€Šby implementing another type of model or NLP taskâ€Š-â€Šthis tutorial still an excellent starter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dqprtoxldNB",
        "colab_type": "text"
      },
      "source": [
        "### Util function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9wLAjDxldNC",
        "colab_type": "text"
      },
      "source": [
        "Function to set the seed for generating random numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPY6c8kcldNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_all(seed_value):\n",
        "    random.seed(seed_value) # Python\n",
        "    np.random.seed(seed_value) # cpu vars\n",
        "    torch.manual_seed(seed_value) # cpu  vars\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
        "        torch.backends.cudnn.deterministic = True  #needed\n",
        "        torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlTKseArldNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_all(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4k7ZXZgldNG",
        "colab_type": "text"
      },
      "source": [
        "### Data pre-processing\n",
        "\n",
        "To match pre-training, we have to format the model input sequence in a specific format.\n",
        "To do so, you have to first **tokenize** and then **numericalize** the texts correctly.\n",
        "The difficulty here is that each pre-trained model, that we will fine-tune, requires exactly the same specific pre-processâ€Š-â€Š**tokenization** & **numericalization**â€Š-â€Šthan the pre-process used during the pre-train part.\n",
        "Fortunately, the **tokenizer class** from ``transformers`` provides the correct pre-process tools that correspond to each pre-trained model.\n",
        "\n",
        "In the ``fastai`` library, data pre-processing is done automatically during the creation of the ``DataBunch``. \n",
        "As you will see in the ``DataBunch`` implementation, the **tokenizer** and **numericalizer** are passed in the processor argument under the following format :\n",
        "\n",
        "``processor = [TokenizeProcessor(tokenizer=tokenizer,...), NumericalizeProcessor(vocab=vocab,...)]``\n",
        "\n",
        "Let's first analyse how we can integrate the ``transformers`` **tokenizer** within the ``TokenizeProcessor`` function.\n",
        "\n",
        "#### Custom Tokenizer\n",
        "This part can be a little bit confusing because a lot of classes are wrapped in each other and with similar names.\n",
        "To resume, if we look attentively at the ``fastai`` implementation, we notice thatÂ :\n",
        "1. The [``TokenizeProcessor`` object](https://docs.fast.ai/text.data.html#TokenizeProcessor) takes as ``tokenizer`` argument a ``Tokenizer`` object.\n",
        "2. The [``Tokenizer`` object](https://docs.fast.ai/text.transform.html#Tokenizer) takes as ``tok_func`` argument a ``BaseTokenizer`` object.\n",
        "3. The [``BaseTokenizer`` object](https://docs.fast.ai/text.transform.html#BaseTokenizer) implement the function ``tokenizer(t:str) â†’ List[str]`` that take a text ``t`` and returns the list of its tokens.\n",
        "\n",
        "Therefore, we can simply create a new class ``TransformersBaseTokenizer`` that inherits from ``BaseTokenizer`` and overwrite a new ``tokenizer`` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBAxbm1UldNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformersBaseTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n",
        "    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n",
        "        self._pretrained_tokenizer = pretrained_tokenizer\n",
        "        self.max_seq_len = pretrained_tokenizer.max_len\n",
        "        self.model_type = model_type\n",
        "\n",
        "    def __call__(self, *args, **kwargs): \n",
        "        return self\n",
        "\n",
        "    def tokenizer(self, t:str) -> List[str]:\n",
        "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n",
        "        CLS = self._pretrained_tokenizer.cls_token\n",
        "        SEP = self._pretrained_tokenizer.sep_token\n",
        "        if self.model_type in ['roberta']:\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n",
        "        else:\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n",
        "        return [CLS] + tokens + [SEP]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp-TBHMNldNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
        "transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
        "fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJtQORD6ldNK",
        "colab_type": "code",
        "outputId": "23193fed-c301-4ef5-ef74-482b032ae106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "tokenizer_class.pretrained_vocab_files_map"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'merges_file': {'distilroberta-base': 'https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-merges.txt',\n",
              "  'roberta-base': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt',\n",
              "  'roberta-base-openai-detector': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt',\n",
              "  'roberta-large': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt',\n",
              "  'roberta-large-mnli': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-merges.txt',\n",
              "  'roberta-large-openai-detector': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt'},\n",
              " 'vocab_file': {'distilroberta-base': 'https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-vocab.json',\n",
              "  'roberta-base': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json',\n",
              "  'roberta-base-openai-detector': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json',\n",
              "  'roberta-large': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json',\n",
              "  'roberta-large-mnli': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-vocab.json',\n",
              "  'roberta-large-openai-detector': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json'}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byfco-3dldNM",
        "colab_type": "text"
      },
      "source": [
        "In this implementation, be carefull about 3 things :\n",
        "1. As we are not using RNN, we have to limit the sequence length to the model input size.\n",
        "2. Most of the models require special tokens placed at the beginning and end of the sequences.\n",
        "3. Some models like RoBERTa require a space to start the input string. For those models, the encoding methods should be called with ``add_prefix_space`` set to ``True``.\n",
        "\n",
        "Below, you can find the resume of each pre-process requirement for the 5 model types used in this tutorial. You can also find this information on the [HuggingFace documentation](https://huggingface.co/transformers/) in each model section.\n",
        "\n",
        "    bert:    [CLS] + tokens + [SEP] + padding\n",
        "\n",
        "    roberta: [CLS] + prefix_space + tokens + [SEP] + padding\n",
        "\n",
        "    xlm:     [CLS] + tokens + [SEP] + padding\n",
        "\n",
        "    xlnet:   padding + [CLS] + tokens + [SEP]\n",
        "    \n",
        "It is worth noting that we don't add padding in this part of the implementation.Â \n",
        "As we will see later, ``fastai`` manage it automatically during the creation of the ``DataBunch``."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbMHSng2ldNM",
        "colab_type": "text"
      },
      "source": [
        "#### Custom Numericalizer\n",
        "\n",
        "In ``fastai``, [``NumericalizeProcessor``  object](https://docs.fast.ai/text.data.html#NumericalizeProcessor) takes as ``vocab`` argument a [``Vocab`` object](https://docs.fast.ai/text.transform.html#Vocab). \n",
        "From this analyse, we suggest two ways to adapt the fastai numericalizer:\n",
        "1. You can, like decribed in the [Dev Sharma's article](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Section *1. Setting Up the Tokenizer*), retreive the list of tokens and create a ``Vocab`` object.\n",
        "2. Create a new class ``TransformersVocab`` that inherits from ``Vocab`` and overwrite ``numericalize`` and ``textify`` functions.\n",
        "\n",
        "Even if the first solution seems to be simpler, ``Transformers`` does not provide, for all models, a straightforward way to retreive his list of tokens. \n",
        "Therefore, I implemented the second solution, which runs for each model type.\n",
        "It consists of using the functions ``convert_tokens_to_ids`` and ``convert_ids_to_tokens`` in respectively ``numericalize`` and ``textify``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIieullHldNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformersVocab(Vocab):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
        "        super(TransformersVocab, self).__init__(itos = [])\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
        "        \"Convert a list of tokens `t` to their ids.\"\n",
        "        return self.tokenizer.convert_tokens_to_ids(t)\n",
        "        #return self.tokenizer.encode(t)\n",
        "\n",
        "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
        "        \"Convert a list of `nums` to their tokens.\"\n",
        "        nums = np.array(nums).tolist()\n",
        "        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dncZyHbldNO",
        "colab_type": "text"
      },
      "source": [
        "#### Custom processor\n",
        "Now that we have our custom **tokenizer** and **numericalizer**, we can create the custom **processor**. Notice we are passing the ``include_bos = False`` and ``include_eos = False`` options. This is because ``fastai`` adds its own special tokens by default which interferes with the ``[CLS]`` and ``[SEP]`` tokens added by our custom tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmgcgWTqldNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\n",
        "numericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n",
        "\n",
        "tokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n",
        "\n",
        "transformer_processor = [tokenize_processor, numericalize_processor]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX7RV5aMldNR",
        "colab_type": "text"
      },
      "source": [
        "### Setting up the Databunch\n",
        "For the DataBunch creation, you have to pay attention to set the processor argument to our new custom processor ``transformer_processor`` and manage correctly the padding.\n",
        "\n",
        "As mentioned in the HuggingFace documentation, BERT, RoBERTa, XLM, and DistilBERT are models with absolute position embeddings, so it's usually advised to pad the inputs on the right rather than the left. Regarding XLNET, it is a model with relative position embeddings, therefore, you can either pad the inputs on the right or on the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXua4aF-ldNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_first = bool(model_type in ['xlnet'])\n",
        "pad_idx = transformer_tokenizer.pad_token_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1JL5meQldNV",
        "colab_type": "text"
      },
      "source": [
        "There is multible ways to create a DataBunch, in our implementation, we use [the data block API](https://docs.fast.ai/data_block.html#The-data-block-API), which gives more flexibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hBdX7jNldNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "databunch = (TextList.from_df(train, cols='Phrase', processor=transformer_processor)\n",
        "             .split_by_rand_pct(0.1,seed=seed)\n",
        "             .label_from_df(cols= 'Sentiment')\n",
        "             .add_test(test)\n",
        "             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViFqI5S1ldNX",
        "colab_type": "text"
      },
      "source": [
        "Check batch and tokenizer :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjaEP9uwldNX",
        "colab_type": "code",
        "outputId": "d59fa5e8-01e1-4d7e-b5e9-d96eef28256e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "print('[CLS] token :', transformer_tokenizer.cls_token)\n",
        "print('[SEP] token :', transformer_tokenizer.sep_token)\n",
        "print('[PAD] token :', transformer_tokenizer.pad_token)\n",
        "databunch.show_batch()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] token : <s>\n",
            "[SEP] token : </s>\n",
            "[PAD] token : <pad>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>&lt;s&gt; Ä - L RB - Ä City Ä - RR B - Ä reminds Ä us Ä how Ä realistically Ä nuanced Ä a Ä Robert Ä De Ä N iro Ä performance Ä can Ä be Ä when Ä he Ä is Ä not Ä more Ä luc r atively Ä engaged Ä in Ä the Ä shameless Ä self - car ic ature Ä of Ä ` Ä Analy ze Ä This Ä ' Ä - L RB - Ä 1999 Ä - RR B - Ä and Ä ` Ä Analy ze Ä That Ä , Ä ' Ä promised Ä - L RB - Ä or Ä threatened Ä -</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>&lt;s&gt; Ä The Ä real Ä triumph s Ä in Ä Ig by Ä come Ä from Ä Philippe Ä , Ä who Ä makes Ä Oliver Ä far Ä more Ä interesting Ä than Ä the Ä character Ä ' s Ä lines Ä would Ä suggest Ä , Ä and Ä Sar andon Ä , Ä who Ä could Ä n 't Ä be Ä better Ä as Ä a Ä cruel Ä but Ä weird ly Ä lik able Ä WAS P Ä mat ron Ä . &lt;/s&gt;</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>&lt;s&gt; Ä Parker Ä should Ä be Ä comm ended Ä for Ä taking Ä a Ä fresh Ä approach Ä to Ä familiar Ä material Ä , Ä but Ä his Ä determination Ä to Ä remain Ä true Ä to Ä the Ä original Ä text Ä leads Ä him Ä to Ä adopt Ä a Ä somewhat Ä man nered Ä tone Ä ... Ä that Ä ultimately Ä dull s Ä the Ä human Ä tragedy Ä at Ä the Ä story Ä ' s Ä core &lt;/s&gt;</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>&lt;s&gt; Ä It Ä ' s Ä a Ä long Ä way Ä from Ä Orwell Ä ' s Ä dark Ä , Ä intelligent Ä warning Ä cry Ä - L RB - Ä 1984 Ä - RR B - Ä to Ä the Ä empty Ä stud Ä knock about Ä of Ä Equ ilibrium Ä , Ä and Ä what Ä once Ä was Ä conviction Ä is Ä now Ä affect ation Ä . &lt;/s&gt;</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>&lt;s&gt; Ä A Ä different Ä and Ä emotionally Ä reserved Ä type Ä of Ä survival Ä story Ä -- Ä a Ä film Ä less Ä about Ä ref ract ing Ä all Ä of Ä World Ä War Ä II Ä through Ä the Ä specific Ä conditions Ä of Ä one Ä man Ä , Ä and Ä more Ä about Ä that Ä man Ä lost Ä in Ä its Ä midst Ä . &lt;/s&gt;</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tFvN953ldNZ",
        "colab_type": "text"
      },
      "source": [
        "Check batch and numericalizer :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WebYH0dnldNZ",
        "colab_type": "code",
        "outputId": "aeb0b029-1afa-419d-8393-dae5a16b78ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "print('[CLS] id :', transformer_tokenizer.cls_token_id)\n",
        "print('[SEP] id :', transformer_tokenizer.sep_token_id)\n",
        "print('[PAD] id :', pad_idx)\n",
        "test_one_batch = databunch.one_batch()[0]\n",
        "print('Batch shape : ',test_one_batch.shape)\n",
        "print(test_one_batch)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] id : 0\n",
            "[SEP] id : 2\n",
            "[PAD] id : 1\n",
            "Batch shape :  torch.Size([16, 79])\n",
            "tensor([[    0,   111,   574,  ...,    76,   479,     2],\n",
            "        [    0,    33,     7,  ...,     1,     1,     1],\n",
            "        [    0,   318,    47,  ...,     1,     1,     1],\n",
            "        ...,\n",
            "        [    0,     5,  2156,  ...,     1,     1,     1],\n",
            "        [    0,    33, 30291,  ...,     1,     1,     1],\n",
            "        [    0, 45518, 10730,  ...,     1,     1,     1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRZfiHNQldNb",
        "colab_type": "text"
      },
      "source": [
        "### Custom model\n",
        "As mentioned [here](https://github.com/huggingface/transformers#models-always-output-tuples), every model's forward method always outputs a ``tuple`` with various elements depending on the model and the configuration parameters. In our case, we are interested to access only to the logits.Â \n",
        "One way to access them is to create a custom model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEeOV4wmldNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining our model architecture \n",
        "class CustomTransformerModel(nn.Module):\n",
        "    def __init__(self, transformer_model: PreTrainedModel):\n",
        "        super(CustomTransformerModel,self).__init__()\n",
        "        self.transformer = transformer_model\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \n",
        "        #attention_mask = (input_ids!=1).type(input_ids.type()) # Test attention_mask for RoBERTa\n",
        "        \n",
        "        logits = self.transformer(input_ids,\n",
        "                                attention_mask = attention_mask)[0]   \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAY2hSruldNc",
        "colab_type": "text"
      },
      "source": [
        "To make our transformers adapted to multiclass classification, before loading the pre-trained model, we need to precise the number of labels. To do so, you can modify the config instance or either modify like in [Keita Kurita's article](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/) (Section: *Initializing the Learner*) the ``num_labels`` argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXGvsVdQldNd",
        "colab_type": "code",
        "outputId": "b007e5e5-248a-4abe-c4a8-18a1199d8f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "config = config_class.from_pretrained(pretrained_model_name)\n",
        "config.num_labels = 5\n",
        "config.use_bfloat16 = use_fp16\n",
        "print(config)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 5,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yL4wCCpldNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n",
        "# transformer_model = model_class.from_pretrained(pretrained_model_name, num_labels = 5)\n",
        "\n",
        "custom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxbOFSDMldNh",
        "colab_type": "text"
      },
      "source": [
        "### LearnerÂ : Custom Optimizer / CustomÂ Metric\n",
        "In ``pytorch-transformers``, HuggingFace had implemented two specific optimizers â€Š-â€Š BertAdam and OpenAIAdam â€Š-â€Š that have been replaced by a single AdamW optimizer.\n",
        "This optimizer matches Pytorch Adam optimizer Api, therefore, it becomes straightforward to integrate it within ``fastai``.\n",
        "It is worth noting that for reproducing BertAdam specific behavior, you have to set ``correct_bias = False``.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmi9f2wmldNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.callbacks import *\n",
        "from transformers import AdamW\n",
        "\n",
        "learner = Learner(databunch, \n",
        "                  custom_transformer_model, \n",
        "                  opt_func = lambda input: AdamW(input,correct_bias=False), \n",
        "                  metrics=[accuracy])\n",
        "\n",
        "# Show graph of learner stats and metrics after each epoch.\n",
        "learner.callbacks.append(ShowGraph(learner))\n",
        "\n",
        "# Put learn in FP16 precision mode. --> Seems to not working\n",
        "if use_fp16: learner = learner.to_fp16()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlesiwb5ldNj",
        "colab_type": "text"
      },
      "source": [
        "### Discriminative Fine-tuning and Gradual unfreezing (Optional)\n",
        "To use **discriminative layer training** and **gradual unfreezing**, ``fastai`` provides one tool that allows to \"split\" the structure model into groups. An instruction to perform that \"split\" is described in the fastai documentation [here](https://docs.fast.ai/basic_train.html#Discriminative-layer-training).\n",
        "\n",
        "Unfortunately,  the model architectures are too different to create a unique generic function that can \"split\" all the model types in a convenient way. Thereby, you will have to implement a custom \"split\" for each different model architecture.\n",
        "\n",
        "For example, if we use the RobBERTa model and that we observe his architecture by making ``print(learner.model)``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BOgl9skldNk",
        "colab_type": "code",
        "outputId": "d66fb6e5-5c3d-48f4-c36e-d23912616fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(learner.model)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CustomTransformerModel(\n",
            "  (transformer): RobertaForSequenceClassification(\n",
            "    (roberta): RobertaModel(\n",
            "      (embeddings): RobertaEmbeddings(\n",
            "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "        (token_type_embeddings): Embedding(1, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (classifier): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (out_proj): Linear(in_features=768, out_features=5, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeBD3nEwldNm",
        "colab_type": "text"
      },
      "source": [
        "We can decide to divide the model in 14 blocksÂ :\n",
        "* 1 Embedding\n",
        "* 12 transformer\n",
        "* 1 classifier\n",
        "\n",
        "In this case, we can split our model in this wayÂ :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3_v839ildNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For DistilBERT\n",
        "# list_layers = [learner.model.transformer.distilbert.embeddings,\n",
        "#                learner.model.transformer.distilbert.transformer.layer[0],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[1],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[2],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[3],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[4],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[5],\n",
        "#                learner.model.transformer.pre_classifier]\n",
        "\n",
        "# For roberta-base\n",
        "list_layers = [learner.model.transformer.roberta.embeddings,\n",
        "              learner.model.transformer.roberta.encoder.layer[0],\n",
        "              learner.model.transformer.roberta.encoder.layer[1],\n",
        "              learner.model.transformer.roberta.encoder.layer[2],\n",
        "              learner.model.transformer.roberta.encoder.layer[3],\n",
        "              learner.model.transformer.roberta.encoder.layer[4],\n",
        "              learner.model.transformer.roberta.encoder.layer[5],\n",
        "              learner.model.transformer.roberta.encoder.layer[6],\n",
        "              learner.model.transformer.roberta.encoder.layer[7],\n",
        "              learner.model.transformer.roberta.encoder.layer[8],\n",
        "              learner.model.transformer.roberta.encoder.layer[9],\n",
        "              learner.model.transformer.roberta.encoder.layer[10],\n",
        "              learner.model.transformer.roberta.encoder.layer[11],\n",
        "              learner.model.transformer.roberta.pooler]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HafpXhNxldNp",
        "colab_type": "text"
      },
      "source": [
        "Check groups : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WqyN_QJldNp",
        "colab_type": "code",
        "outputId": "acf251df-ca6f-45b6-dc2f-51a47d68f3d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learner.split(list_layers)\n",
        "num_groups = len(learner.layer_groups)\n",
        "print('Learner split in',num_groups,'groups')\n",
        "print(learner.layer_groups)\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learner split in 14 groups\n",
            "[Sequential(\n",
            "  (0): Embedding(50265, 768, padding_idx=1)\n",
            "  (1): Embedding(514, 768, padding_idx=1)\n",
            "  (2): Embedding(1, 768)\n",
            "  (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (4): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Tanh()\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=5, bias=True)\n",
            ")]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VuXoM3mldNq",
        "colab_type": "text"
      },
      "source": [
        "Note that I didn't found any document that has studied the influence of **Discriminative Fine-tuning** and **Gradual unfreezing** or even **Slanted Triangular Learning Rates** with transformers. Therefore, using these tools does not guarantee better results. If you found any interesting documents, please let us know in the comment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efhc7s0jldNr",
        "colab_type": "text"
      },
      "source": [
        "### Train\n",
        "Now we can finally use all the fastai build-in features to train our model. Like the ULMFiT method, we will use **Slanted Triangular Learning Rates**, **Discriminate Learning Rate** and **gradually unfreeze the model**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qt65gAhldNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.save('untrain')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ31uhVqldNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_all(seed)\n",
        "learner.load('untrain');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0aZGzOpldNu",
        "colab_type": "text"
      },
      "source": [
        "Therefore, we first freeze all the groups but the classifier withÂ :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF5cN5GOldNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.freeze_to(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKGHyd0mldNw",
        "colab_type": "text"
      },
      "source": [
        "We check which layer are trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fQSJ0qcldNw",
        "colab_type": "code",
        "outputId": "bb088f61-2e18-4f9b-dff9-5014ce7f029a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learner.summary()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomTransformerModel\n",
              "======================================================================\n",
              "Layer (type)         Output Shape         Param #    Trainable \n",
              "======================================================================\n",
              "Embedding            [79, 768]            38,603,520 False     \n",
              "______________________________________________________________________\n",
              "Embedding            [79, 768]            394,752    False     \n",
              "______________________________________________________________________\n",
              "Embedding            [79, 768]            768        False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 79, 79]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 79, 79]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 79, 79]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 79, 79]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 79, 79]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 79, 79]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 79, 79]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 79, 79]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 79, 79]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 79, 79]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 79, 79]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 79, 79]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [79, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [79, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [79, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [768]                590,592    True      \n",
              "______________________________________________________________________\n",
              "Tanh                 [768]                0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [768]                590,592    True      \n",
              "______________________________________________________________________\n",
              "Dropout              [768]                0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [5]                  3,845      True      \n",
              "______________________________________________________________________\n",
              "\n",
              "Total params: 125,240,069\n",
              "Total trainable params: 1,185,029\n",
              "Total non-trainable params: 124,055,040\n",
              "Optimized with f02666b5e18\n",
              "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
              "Loss function : FlattenedLoss\n",
              "======================================================================\n",
              "Callbacks functions applied \n",
              "    ShowGraph"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DfEfDPKldNy",
        "colab_type": "text"
      },
      "source": [
        "For **Slanted Triangular Learning Rates** you have to use the function ``one_cycle``. For more information please check the fastai documentation [here](https://docs.fast.ai/callbacks.one_cycle.html).Â \n",
        "\n",
        "To use our ``one_cycle`` we will need an optimum learning rate. We can find this learning rate by using a learning rate finder which can be called by using ``lr_find``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z512ncHFldNy",
        "colab_type": "code",
        "outputId": "4526ede0-dc31-4aef-d161-62ed627383aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "learner.lr_find()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNAcM_WEldN0",
        "colab_type": "code",
        "outputId": "c73ce611-d79d-403c-fa31-b16e4cef73c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "learner.recorder.plot(skip_end=7,suggestion=True)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Min numerical gradient: 3.63E-03\n",
            "Min loss divided by 10: 4.37E-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnzY0__aldN1",
        "colab_type": "text"
      },
      "source": [
        "We will pick a value a bit before the minimum, where the loss still improves. Here 2x10^-3 seems to be a good value.\n",
        "\n",
        "Next we will use ``fit_one_cycle`` with the chosen learning rate as the maximum learning rate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRlbtAm7ldN2",
        "colab_type": "code",
        "outputId": "be30b553-10ec-40a0-9462-303d7e209180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.072603</td>\n",
              "      <td>0.992165</td>\n",
              "      <td>0.598744</td>\n",
              "      <td>07:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD5CAYAAAA3Os7hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoyBhox0ldN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.save('first_cycle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geX98dA7ldN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_all(seed)\n",
        "learner.load('first_cycle');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgrEyMvVldN7",
        "colab_type": "text"
      },
      "source": [
        "We then unfreeze the second group of layers and repeat the operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK9I6xldldN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.freeze_to(-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-eaONEZldN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPUZJgMYldN-",
        "colab_type": "text"
      },
      "source": [
        "Note here that we use slice to create separate learning rate for each group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1ODVcIGldN-",
        "colab_type": "code",
        "outputId": "436b9190-9101-4e18-ce25-a913ef1fe819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "source": [
        "learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.914303</td>\n",
              "      <td>0.907117</td>\n",
              "      <td>0.633282</td>\n",
              "      <td>09:03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHz9ZrNXnOy8GAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmIQe8gJldOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.save('second_cycle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp-MrexXldOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_all(seed)\n",
        "learner.load('second_cycle');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppEwVXLDldOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.freeze_to(-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqIpR0WpldOE",
        "colab_type": "code",
        "outputId": "091fbb96-5c42-4f52-c897-adb335b72146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "source": [
        "learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.860208</td>\n",
              "      <td>0.872932</td>\n",
              "      <td>0.646034</td>\n",
              "      <td>10:39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w9mbWusvQVrJgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yagg6SZgldOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.save('third_cycle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMRaik4GldOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_all(seed)\n",
        "learner.load('third_cycle');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P9h-45TldOH",
        "colab_type": "text"
      },
      "source": [
        "Here, we unfreeze all the groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e0mYmC8ldOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.unfreeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41wvnAIkldOJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "47df918e-68c5-4bb2-cf34-2d5aac36fed2"
      },
      "source": [
        "learner.fit_one_cycle(2, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.709644</td>\n",
              "      <td>0.714858</td>\n",
              "      <td>0.699795</td>\n",
              "      <td>30:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.668368</td>\n",
              "      <td>0.696342</td>\n",
              "      <td>0.706908</td>\n",
              "      <td>28:58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD7CAYAAAB68m/qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAWx38ys9TkA+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y78ubItcldOL",
        "colab_type": "text"
      },
      "source": [
        "### Creating prediction\n",
        "Now that the model is trained, we want to generate predictions from the test dataset.\n",
        "\n",
        "As specified in *Keita Kurita's* [article](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/), as the function ``get_preds`` does not return elements in order by default, you will have to resort the elements into their correct order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvxl1RBTldOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    the get_preds method does not yield the elements in order by default\n",
        "    we borrow the code from the RNNLearner to resort the elements into their correct order\n",
        "    \"\"\"\n",
        "    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n",
        "    sampler = [i for i in databunch.dl(ds_type).sampler]\n",
        "    reverse_sampler = np.argsort(sampler)\n",
        "    return preds[reverse_sampler, :]\n",
        "\n",
        "test_preds = get_preds_as_nparray(DatasetType.Test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcOgzu7HJ7Mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission = pd.read_csv(\"/content/sampleSubmission.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9HcZtC6cp1c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1a28f1b7-9a29-4763-80a5-336262f4c9f2"
      },
      "source": [
        "len(test_preds)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66292"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i9FomheUrz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission['Sentiment'] = np.argmax(test_preds,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH-gXSm9W0g4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission.to_csv(\"/content/predictions.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0zmAh-UldON",
        "colab_type": "text"
      },
      "source": [
        "We check the order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byrEu7j5ldON",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "fe358900-e404-49ec-b53f-53c2095e1be9"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>156061</td>\n",
              "      <td>8545</td>\n",
              "      <td>An intermittently pleasing but mostly routine ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>156062</td>\n",
              "      <td>8545</td>\n",
              "      <td>An intermittently pleasing but mostly routine ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>156063</td>\n",
              "      <td>8545</td>\n",
              "      <td>An</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>156064</td>\n",
              "      <td>8545</td>\n",
              "      <td>intermittently pleasing but mostly routine effort</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>156065</td>\n",
              "      <td>8545</td>\n",
              "      <td>intermittently pleasing but mostly routine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  SentenceId                                             Phrase\n",
              "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
              "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
              "2    156063        8545                                                 An\n",
              "3    156064        8545  intermittently pleasing but mostly routine effort\n",
              "4    156065        8545         intermittently pleasing but mostly routine"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRG9Wq05ldOO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "7f92cb9d-3200-40ee-a0f2-06ed99649247"
      },
      "source": [
        "sample_submission.head()"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>156061</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>156062</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>156063</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>156064</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>156065</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  Sentiment\n",
              "0    156061          2\n",
              "1    156062          2\n",
              "2    156063          2\n",
              "3    156064          2\n",
              "4    156065          2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns9flKGEldOS",
        "colab_type": "text"
      },
      "source": [
        "We can now submit our predictions to Kaggle !  In our example, without playing too much with the parameters, we get a score of ..., which leads us to the X position on the leaderboard! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGBcFpTMldOS",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this NoteBook, I explain how to combine the ``transformers`` library with the beloved ``fastai`` library. It aims to make you understand where to look and modify both libraries to make them work together. Likely, it allows you to use **Slanted Triangular Learning Rates**, **Discriminate Learning Rate** and even **Gradual Unfreezing**. As a result, without even tunning the parameters, you can obtain rapidly state-of-the-art results.\n",
        "\n",
        "This year, the transformers became an essential tool to NLP. Because of that, I think that pre-trained transformers architectures will be integrated soon to future versions of fastai. Meanwhile, this tutorial is a good starter.\n",
        "\n",
        "I hope you enjoyed this first article and found it useful.Â \n",
        "Thanks for reading and don't hesitate in leaving questions or suggestions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxRbLAKldOS",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "* Hugging Face, Transformers GitHub (Nov 2019), [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)\n",
        "* Fast.ai, Fastai documentation (Nov 2019), [https://docs.fast.ai/text.html](https://docs.fast.ai/text.html)\n",
        "* Jeremy Howard & Sebastian Ruder, Universal Language Model Fine-tuning for Text Classification (May 2018), [https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146)\n",
        "* Keita Kurita's articleÂ : [A Tutorial to Fine-Tuning BERT with Fast AI](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/)Â (May 2019)\n",
        "* Dev Sharma's articleÂ : [Using RoBERTa with Fastai for NLP](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Sep 2019)"
      ]
    }
  ]
}